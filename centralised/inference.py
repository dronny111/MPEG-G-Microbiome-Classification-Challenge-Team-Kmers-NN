#!/usr/bin/env python
import os, json, argparse, warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from codecarbon import EmissionsTracker

from .config import *
from .utils import set_seed, load_data, free_memory
from torch.nn import functional as F

class MicrobiomeTabularConvNet(nn.Module):
    def __init__(self, n_classes=1):
        super(MicrobiomeTabularConvNet, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=1, padding=1)
        self.bn1 = nn.BatchNorm1d(16)
        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=1, padding=1)
        self.bn2 = nn.BatchNorm1d(32)
        self.fc1 = nn.Linear(19360, 512)
        self.fc2 = nn.Linear(512, 64)
        self.fc3 = nn.Linear(64, n_classes)

    def forward(self, x):
        x = x.unsqueeze(1)  # Add channel dimension
        x = F.relu(self.conv1(x))
        x = self.bn1(x)
        x = F.relu(self.conv2(x))
        x = self.bn2(x)
        x = x.view(x.size(0), -1)  # Flatten
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


def load_checkpoints(dir_path: str) -> list[dict]:
    """Read every *.pth file that matches the naming convention."""
    ckpts = []
    for fn in sorted(os.listdir(dir_path)):
        if fn.startswith("fold") and fn.endswith("_best.pth"):
            ckpts.append(torch.load(os.path.join(dir_path, fn),
                                   weights_only=False))
    return ckpts



def main(args):
    set_seed(SEED, use_cuda=torch.cuda.is_available())

    _, _, _, test_feat = load_data()

    with open(args.meta_path) as fp:
        meta = json.load(fp)
    cont_features = meta["cont_features"]



    X_test = test_feat[cont_features].values.astype(np.float32)

    test_dataset = TensorDataset(torch.from_numpy(X_test))
    test_loader = DataLoader(test_dataset,
                            batch_size=BATCH_SIZE,
                            shuffle=False,
                            pin_memory=True)

    checkpoints = load_checkpoints(args.ckpt_dir)
    print(f"ðŸ”Ž Found {len(checkpoints)} checkpoints.")

    model = MicrobiomeTabularConvNet(n_classes=len(TARGET_LABELS)).to(DEVICE)

    all_probs = []          
    for i, ckpt in enumerate(checkpoints):
        model.load_state_dict(ckpt["model"])
        model.eval()
        probs_fold = []

        with torch.no_grad():
            with EmissionsTracker(project_name="centralised_inference") as trk:
                trk.start()
                for batch in test_loader:
                    xb = batch[0].to(DEVICE)
                    logits = model(xb)
                    probs = torch.softmax(logits, dim=1)
                    probs_fold.append(probs.cpu().numpy())
                trk.stop()
        probs_fold = np.concatenate(probs_fold, axis=0) 
        all_probs.append(probs_fold)

        print(
            f"[{i+1}/{len(checkpoints)}] "
            f"{probs_fold.shape[0]} samples, {probs_fold.shape[1]} classes"
        )

    ensemble = np.mean(np.stack(all_probs, axis=0), axis=0)

    sub_df = test_feat[["file"]].copy()
    sub_df[TARGET_LABELS] = ensemble
    sub_df["file"] = sub_df["file"].str.replace(".fastq", "")
    sub_df.rename(columns={"file": "filename"}, inplace=True)

    out_path = os.path.join(args.out_dir, "kmers_centralised_submission_nn.csv")
    os.makedirs(args.out_dir, exist_ok=True)
    sub_df[['filename', TARGET_LABELS]].to_csv(out_path, index=False)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run inference on the MPEGâ€‘Kmer test set."
    )
    parser.add_argument(
        "--ckpt_dir",
        type=str,
        default="checkpoints",
        help="Folder that contains the perâ€‘fold *.pth files.",
    )
    parser.add_argument(
        "--meta_path",
        type=str,
        default="artifacts/meta.json",
        help="Path to the meta.json generated by train.py",
    )
    parser.add_argument(
        "--out_dir",
        type=str,
        default="submission",
        help="Folder where the final CSV will be saved.",
    )
    args = parser.parse_args()
    main(args)
